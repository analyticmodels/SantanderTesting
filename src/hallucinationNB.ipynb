{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hcyohxmzwe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 1271\n",
      "Deduplicated sentences: 255\n",
      "Duplicates removed: 1016\n",
      "Saved to: ../data/baseQuestions.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Read sentences from the file\n",
    "data_file = Path('../data/questions.txt')\n",
    "with open(data_file, 'r', encoding='utf-8') as f:\n",
    "    sentences = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'question': sentences})\n",
    "\n",
    "# Remove duplicates\n",
    "df_dedup = df.drop_duplicates(subset=['question'], keep='first')\n",
    "\n",
    "# Save as parquet using fastparquet engine to avoid PyArrow conflicts\n",
    "output_file = Path('../data/baseQuestions.parquet')\n",
    "df_dedup.to_parquet(output_file, engine='fastparquet', index=False)\n",
    "\n",
    "print(f\"Total sentences: {len(df)}\")\n",
    "print(f\"Deduplicated sentences: {len(df_dedup)}\")\n",
    "print(f\"Duplicates removed: {len(df) - len(df_dedup)}\")\n",
    "print(f\"Saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tgsh97a7v3h",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nfrom pathlib import Path\nimport ollama\nimport random\n\n# Load base questions using fastparquet engine\nbase_questions_file = Path('../data/baseQuestions.parquet')\ndf_base = pd.read_parquet(base_questions_file, engine='fastparquet')\nbase_questions = df_base['question'].tolist()\n\nprint(f\"Loaded {len(base_questions)} base questions\")\n\n# Generate hallucination-inducing questions using Ollama with Gemma 3\nhallucination_questions = []\ntarget_count = 1000\n\n# Sample base questions to work with\nsample_size = min(255, len(base_questions))\nsampled_questions = random.sample(base_questions, sample_size)\n\nfor i, base_question in enumerate(sampled_questions):\n    # Generate multiple variations per base question\n    questions_per_base = (target_count // sample_size) + (1 if i < (target_count % sample_size) else 0)\n    \n    prompt = f\"\"\"Based on this question: \"{base_question}\"\n\nGenerate {questions_per_base} new questions designed to cause LLM hallucinations (not just incorrect answers, but actual hallucinations involving confabulation of plausible-sounding but false details).\n\nUse these techniques:\n- Replace entities with plausible but potentially non-existent names\n- Add specific numerical details (dates, percentages, amounts, limits)\n- Combine real concepts with fabricated specifics\n- Include precise policy/feature details that require exact knowledge\n- Use obscure or ambiguous entity names that sound legitimate\n\nReturn ONLY the questions, one per line, no numbering or extra text.\"\"\"\n\n    response = ollama.chat(\n        model=\"gemma3\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    \n    # Extract questions from response\n    generated = response['message']['content'].strip().split('\\n')\n    generated = [q.strip() for q in generated if q.strip() and not q.strip()[0].isdigit()]\n    \n    hallucination_questions.extend(generated[:questions_per_base])\n    print(f\"Generated {len(generated[:questions_per_base])} questions from base question {i+1}/{len(sampled_questions)}\")\n\n# Ensure we have exactly 50 questions\nhallucination_questions = hallucination_questions[:target_count]\n\nprint(f\"\\nTotal hallucination questions generated: {len(hallucination_questions)}\")\nprint(\"Now generating answers for each question...\")\n\n# Generate answers for each hallucination question\nresults = []\nfor i, question in enumerate(hallucination_questions):\n    response = ollama.chat(\n        model=\"gemma3\",\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n    \n    answer = response['message']['content'].strip()\n    results.append({\n        'question': question,\n        'answer': answer\n    })\n    \n    if (i + 1) % 10 == 0:\n        print(f\"Generated answers for {i + 1}/{len(hallucination_questions)} questions\")\n\n# Create DataFrame and save\ndf_hallucination = pd.DataFrame(results)\noutput_file = Path('../data/hallucinationQuestions_gemma3.parquet')\ndf_hallucination.to_parquet(output_file, engine='fastparquet', index=False)\n\nprint(f\"\\nTotal question-answer pairs generated: {len(results)}\")\nprint(f\"Saved to: {output_file}\")\nprint(f\"Columns: {df_hallucination.columns.tolist()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "m2soxprs26j",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 question-answer pairs\n",
      "Processed 10/10 pairs\n",
      "\n",
      "=== Hallucination Detection Results ===\n",
      "Total pairs evaluated: 10\n",
      "PASS (faithful): 5 (50.0%)\n",
      "FAIL (hallucination detected): 4 (40.0%)\n",
      "UNKNOWN: 1 (10.0%)\n",
      "\n",
      "Saved to: ../data/hallucinationDetection_lynx.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import ollama\n",
    "import json\n",
    "\n",
    "# Patronus Lynx hallucination detection using Ollama\n",
    "# Model: tensortemplar/patronus-lynx:8b-instruct-q4_k_m\n",
    "# See: https://huggingface.co/PatronusAI/Llama-3-Patronus-Lynx-8B-Instruct\n",
    "\n",
    "# Load the question-answer pairs generated by Gemma 3\n",
    "#input_file = Path('../data/hallucinationQuestions_gemma3.parquet')\n",
    "input_file = Path('../data/hallucinationQuestionsAnth.parquet')\n",
    "df = pd.read_parquet(input_file, engine='fastparquet')\n",
    "df=df[:10]\n",
    "print(f\"Loaded {len(df)} question-answer pairs\")\n",
    "\n",
    "# Lynx prompt template for hallucination detection\n",
    "LYNX_PROMPT_TEMPLATE = \"\"\"Given the following QUESTION, DOCUMENT and ANSWER you must analyze the provided answer and determine whether it is faithful to the contents of the DOCUMENT.\n",
    "\n",
    "The ANSWER must not offer new information beyond the context provided in the DOCUMENT.\n",
    "\n",
    "The ANSWER also must not contradict information provided in the DOCUMENT.\n",
    "\n",
    "Output your final verdict by strictly following this format: \"PASS\" if the answer is faithful to the DOCUMENT and \"FAIL\" if the answer is not faithful to the DOCUMENT.\n",
    "\n",
    "Show your reasoning.\n",
    "\n",
    "--\n",
    "QUESTION (THIS DOES NOT COUNT AS BACKGROUND INFORMATION):\n",
    "{question}\n",
    "\n",
    "--\n",
    "DOCUMENT:\n",
    "{document}\n",
    "\n",
    "--\n",
    "ANSWER:\n",
    "{answer}\n",
    "\n",
    "--\n",
    "Your output should be in JSON FORMAT with the keys \"REASONING\" and \"SCORE\":\n",
    "{{\"REASONING\": <your reasoning as bullet points>, \"SCORE\": <your final score>}}\n",
    "\"\"\"\n",
    "\n",
    "def detect_hallucination(question: str, answer: str, document: str = \"\") -> dict:\n",
    "    \"\"\"\n",
    "    Use Patronus Lynx to detect if an answer contains hallucinations.\n",
    "    \n",
    "    Args:\n",
    "        question: The question that was asked\n",
    "        answer: The answer to evaluate\n",
    "        document: Reference document/context (if empty, we're checking for unsupported claims)\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'reasoning', 'score', and 'raw_response'\n",
    "    \"\"\"\n",
    "    # If no document provided, use a minimal context indicating no ground truth available\n",
    "    if not document:\n",
    "        document = \"No reference document provided. Evaluate if the answer makes specific claims that cannot be verified.\"\n",
    "    \n",
    "    prompt = LYNX_PROMPT_TEMPLATE.format(\n",
    "        question=question,\n",
    "        document=document,\n",
    "        answer=answer\n",
    "    )\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=\"tensortemplar/patronus-lynx:8b-instruct-q4_k_m\",  # or use full name: tensortemplar/patronus-lynx:8b-instruct-q4_k_m\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    raw_response = response['message']['content'].strip()\n",
    "    \n",
    "    # Try to parse JSON from response\n",
    "    try:\n",
    "        # Find JSON in response (it might have extra text)\n",
    "        json_start = raw_response.find('{')\n",
    "        json_end = raw_response.rfind('}') + 1\n",
    "        if json_start != -1 and json_end > json_start:\n",
    "            json_str = raw_response[json_start:json_end]\n",
    "            parsed = json.loads(json_str)\n",
    "            return {\n",
    "                'reasoning': parsed.get('REASONING', ''),\n",
    "                'score': parsed.get('SCORE', 'UNKNOWN'),\n",
    "                'raw_response': raw_response\n",
    "            }\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    # Fallback: extract score from text\n",
    "    score = 'UNKNOWN'\n",
    "    if 'FAIL' in raw_response.upper():\n",
    "        score = 'FAIL'\n",
    "    elif 'PASS' in raw_response.upper():\n",
    "        score = 'PASS'\n",
    "    \n",
    "    return {\n",
    "        'reasoning': raw_response,\n",
    "        'score': score,\n",
    "        'raw_response': raw_response\n",
    "    }\n",
    "\n",
    "# Run hallucination detection on all Q&A pairs\n",
    "results = []\n",
    "for i, row in df.iterrows():\n",
    "    result = detect_hallucination(\n",
    "        question=row['question'],\n",
    "        answer=row['answer']\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'question': row['question'],\n",
    "        'answer': row['answer'],\n",
    "        'hallucination_score': result['score'],\n",
    "        'reasoning': result['reasoning'],\n",
    "        'raw_lynx_response': result['raw_response']\n",
    "    })\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Processed {i + 1}/{len(df)} pairs\")\n",
    "\n",
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save results\n",
    "output_file = Path('../data/hallucinationDetection_lynx.parquet')\n",
    "df_results.to_parquet(output_file, engine='fastparquet', index=False)\n",
    "\n",
    "# Summary statistics\n",
    "pass_count = (df_results['hallucination_score'] == 'PASS').sum()\n",
    "fail_count = (df_results['hallucination_score'] == 'FAIL').sum()\n",
    "unknown_count = (df_results['hallucination_score'] == 'UNKNOWN').sum()\n",
    "\n",
    "print(f\"\\n=== Hallucination Detection Results ===\")\n",
    "print(f\"Total pairs evaluated: {len(df_results)}\")\n",
    "print(f\"PASS (faithful): {pass_count} ({100*pass_count/len(df_results):.1f}%)\")\n",
    "print(f\"FAIL (hallucination detected): {fail_count} ({100*fail_count/len(df_results):.1f}%)\")\n",
    "print(f\"UNKNOWN: {unknown_count} ({100*unknown_count/len(df_results):.1f}%)\")\n",
    "print(f\"\\nSaved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "mxx8vsp4l3",
   "source": "import pandas as pd\nfrom pathlib import Path\nfrom anthropic import Anthropic\nimport httpx\nimport random\n\n# Initialize Anthropic client with custom httpx client to handle SSL issues\nhttp_client = httpx.Client(verify=False)\nclient = Anthropic(\n    api_key=\"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\",\n    http_client=http_client\n)\n\n# Load base questions using fastparquet engine\nbase_questions_file = Path('../data/baseQuestions.parquet')\ndf_base = pd.read_parquet(base_questions_file, engine='fastparquet')\nbase_questions = df_base['question'].tolist()\n\nprint(f\"Loaded {len(base_questions)} base questions\")\n\n# Generate hallucination-inducing questions\nhallucination_questions = []\ntarget_count = 50\n\n# Sample base questions to work with\nsample_size = min(10, len(base_questions))\nsampled_questions = random.sample(base_questions, sample_size)\n\nfor i, base_question in enumerate(sampled_questions):\n    # Generate multiple variations per base question\n    questions_per_base = (target_count // sample_size) + (1 if i < (target_count % sample_size) else 0)\n    \n    prompt = f\"\"\"Based on this question: \"{base_question}\"\n\nGenerate {questions_per_base} new questions designed to cause LLM hallucinations (not just incorrect answers, but actual hallucinations involving confabulation of plausible-sounding but false details).\n\nUse these techniques:\n- Replace entities with plausible but potentially non-existent names\n- Add specific numerical details (dates, percentages, amounts, limits)\n- Combine real concepts with fabricated specifics\n- Include precise policy/feature details that require exact knowledge\n- Use obscure or ambiguous entity names that sound legitimate\n\nReturn ONLY the questions, one per line, no numbering or extra text.\"\"\"\n\n    response = client.messages.create(\n        model=\"claude-sonnet-4-5-20250929\",\n        max_tokens=2000,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    \n    # Extract questions from response\n    generated = response.content[0].text.strip().split('\\n')\n    generated = [q.strip() for q in generated if q.strip() and not q.strip()[0].isdigit()]\n    \n    hallucination_questions.extend(generated[:questions_per_base])\n    print(f\"Generated {len(generated[:questions_per_base])} questions from base question {i+1}/{len(sampled_questions)}\")\n\n# Ensure we have exactly target_count questions\nhallucination_questions = hallucination_questions[:target_count]\n\nprint(f\"\\nTotal hallucination questions generated: {len(hallucination_questions)}\")\nprint(\"Now generating answers for each question...\")\n\n# Generate answers for each hallucination question\nresults = []\nfor i, question in enumerate(hallucination_questions):\n    response = client.messages.create(\n        model=\"claude-sonnet-4-5-20250929\",\n        max_tokens=2000,\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )\n    \n    answer = response.content[0].text.strip()\n    results.append({\n        'question': question,\n        'answer': answer\n    })\n    \n    if (i + 1) % 10 == 0:\n        print(f\"Generated answers for {i + 1}/{len(hallucination_questions)} questions\")\n\n# Create DataFrame and save\ndf_hallucination = pd.DataFrame(results)\noutput_file = Path('../data/hallucinationQuestionsAnth.parquet')\ndf_hallucination.to_parquet(output_file, engine='fastparquet', index=False)\n\nprint(f\"\\nTotal question-answer pairs generated: {len(results)}\")\nprint(f\"Saved to: {output_file}\")\nprint(f\"Columns: {df_hallucination.columns.tolist()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}